# EE569 HOMEWORK ASSIGNMENT 5
# DATE: Aprile 14th 2021
# NAME: Siyu Li
# ID:2455870216
# E-mail:lisiyu@usc.edu
###########################################################
# mnist
# implement pixelhop++ by cwSaab
from pixelhop import Pixelhop
import numpy as np
from skimage.util import view_as_windows
import pickle
from skimage.measure import block_reduce
import xgboost as xgb
import time
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist,fashion_mnist
import warnings, gc
import seaborn as sns
np.random.seed(1)

# Preprocess
N_Train_Reduced = 10000    # 10000
N_Train_Full = 60000     # 50000
N_Test = 10000            # 10000

BS = 10000 # batch size


def shuffle_data(X, y):
    shuffle_idx = np.random.permutation(y.size)
    X = X[shuffle_idx]
    y = y[shuffle_idx]
    return X, y


def select_balanced_subset(images, labels, use_num_images):
    '''
    select equal number of images from each classes
    '''
    num_total, H, W, C = images.shape
    num_class = np.unique(labels).size
    num_per_class = int(use_num_images / num_class)

    # Shuffle
    images, labels = shuffle_data(images, labels)

    selected_images = np.zeros((use_num_images, H, W, C))
    selected_labels = np.zeros(use_num_images)

    for i in range(num_class):
        selected_images[i * num_per_class:(i + 1) * num_per_class] = images[labels == i][:num_per_class]
        selected_labels[i * num_per_class:(i + 1) * num_per_class] = np.ones((num_per_class)) * i

    # Shuffle again
    selected_images, selected_labels = shuffle_data(selected_images, selected_labels)

    return selected_images, selected_labels

def Shrink(X, shrinkArg):
    #---- max pooling----
    pool = shrinkArg['pool']
    # TODO: fill in the rest of max pooling
    X=block_reduce(X,(1,pool,pool,1),np.max)
    
    #---- neighborhood construction
    win = shrinkArg['win']
    stride = shrinkArg['stride']
    pad = shrinkArg['pad']
    ch=X.shape[-1]
    # TODO: fill in the rest of neighborhood construction
    if pad>0:
        X=np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)),'reflect')
    X=view_as_windows(X,(1,win,win,ch),(1,stride,stride,ch))
    return X.reshape(X.shape[0],X.shape[1],X.shape[2],-1)

# example callback function for how to concate features from different hops
def Concat(X, concatArg):
    return X

def get_feat(X, num_layers=3):
    output = p2.transform_singleHop(X,layer=0)
    if num_layers>1:
        for i in range(num_layers-1):
            output = p2.transform_singleHop(output, layer=i+1)
    return output
if __name__ == "__main__":
    warnings.filterwarnings("ignore")
    # ---------- Load MNIST data and split ----------
    (x_train, y_train), (x_test,y_test) = mnist.load_data()


    # -----------Data Preprocessing-----------
    x_train = np.asarray(x_train,dtype='float32')[:,:,:,np.newaxis]
    x_test = np.asarray(x_test,dtype='float32')[:,:,:,np.newaxis]
    y_train = np.asarray(y_train,dtype='int')
    y_test = np.asarray(y_test,dtype='int')

    # if use only 10000 images train pixelhop
    x_train_reduced, y_train_reduced = select_balanced_subset(x_train, y_train, use_num_images=N_Train_Reduced)

    x_train /= 255.0
    x_test /= 255.0


    # -----------Module 1: set PixelHop parameters-----------
    # TODO: fill in this part

    SaabArgs = [{'num_AC_kernels':-1, 'needBias':False, 'cw':False}, 
            {'num_AC_kernels':-1, 'needBias':True, 'cw':True},
            {'num_AC_kernels':-1, 'needBias':True, 'cw':True}]
    shrinkArgs = [{'func':Shrink, 'win':5, 'stride':1, 'pad':2,'pool':1}, 
             {'func': Shrink, 'win':5, 'stride':1, 'pad':0, 'pool':2},
             {'func': Shrink, 'win':5, 'stride':1,'pad':0, 'pool':2}]
    concatArg = {'func':Concat}
    TH1_list=[0.1,0.01,0.005,0.001,0.0005]
    acc_list=[]
for t in TH1_list:
    # -----------Module 1: Train PixelHop -----------
    # TODO: fill in this part
    
        p2 = Pixelhop(depth=3, TH1=t, TH2=0.001, SaabArgs=SaabArgs, shrinkArgs=shrinkArgs, concatArg=concatArg)
        p2.fit(x_train_reduced)

    
    # --------- Module 2: get only Hop 3 feature for both training set and testing set -----------
    # you can get feature "batch wise" and concatenate them if your memory is restricted
    # TODO: fill in this part
        split_train_arr=[]
    
        for i in range(6):
                temp=get_feat(x_train[i*BS:(i+1)*BS,:,:,:],3)
                split_train_arr.append(temp)
            
        train_hop3_feats=np.concatenate((split_train_arr[0],split_train_arr[1],split_train_arr[2],split_train_arr[3]
                                     ,split_train_arr[4],split_train_arr[5]),axis=0)
    
    
    
    #train_hop3_feats=get_feat(x_train,3)
        test_hop3_feats=get_feat(x_test,3)
    
    ### Get Model Size
        h1=get_feat(x_train_reduced,1).shape
        h2=get_feat(x_train_reduced,2).shape
        h3=get_feat(x_train_reduced,3).shape
    
        k1=h1[3]
        k2=h2[3]
        k3=h3[3]
    
        print('k1: ', k1)
        print('k2: ', k2)
        print('k3: ', k3)
    # --------- Module 2: standardization
        STD = np.std(train_hop3_feats, axis=0, keepdims=1)
        train_hop3_feats = train_hop3_feats/STD ## size: (60000,1,1,70)
    #print(np.shape(train_hop3_feats))
    #STDtest = np.std(test_hop3_feats, axis=0, keepdims=1)
        test_hop3_feats = test_hop3_feats/STD
    
    #---------- Module 3: Train XGBoost classifier on hop3 feature ---------
        
    
        clf = xgb.XGBClassifier(n_jobs=-1,
                            objective='multi:softprob',
                        # tree_method='gpu_hist', gpu_id=None,
                            max_depth=6,n_estimators=100,
                            min_child_weight=5,gamma=5,
                            subsample=0.8,learning_rate=0.1,
                            nthread=8,colsample_bytree=1.0)
        clf.fit(np.squeeze(train_hop3_feats), np.squeeze(y_train))


        
    # TODO: fill in the rest and report accuracy
    
        score = clf.score(np.squeeze(test_hop3_feats), np.squeeze(y_test))
        print('accuracy', score)
        acc_list.append(score)
plt.plot(TH1_list,acc_list)

plt.xlabel('TH1 value')
plt.ylabel('accuracy')

plt.title('TH1 value vs Accuracy Plot')
if __name__ == "__main__":
    warnings.filterwarnings("ignore")
    # ---------- Load MNIST data and split ----------
    (x_train, y_train), (x_test,y_test) = fashion_mnist.load_data()


    # -----------Data Preprocessing-----------
    x_train = np.asarray(x_train,dtype='float32')[:,:,:,np.newaxis]
    x_test = np.asarray(x_test,dtype='float32')[:,:,:,np.newaxis]
    y_train = np.asarray(y_train,dtype='int')
    y_test = np.asarray(y_test,dtype='int')

    # if use only 10000 images train pixelhop
    x_train_reduced, y_train_reduced = select_balanced_subset(x_train, y_train, use_num_images=N_Train_Reduced)

    x_train /= 255.0
    x_test /= 255.0


    # -----------Module 1: set PixelHop parameters-----------
    # TODO: fill in this part

    SaabArgs = [{'num_AC_kernels':-1, 'needBias':False, 'cw':False}, 
            {'num_AC_kernels':-1, 'needBias':True, 'cw':True},
            {'num_AC_kernels':-1, 'needBias':True, 'cw':True}]
    shrinkArgs = [{'func':Shrink, 'win':5, 'stride':1, 'pad':2,'pool':1}, 
             {'func': Shrink, 'win':5, 'stride':1, 'pad':0, 'pool':2},
             {'func': Shrink, 'win':5, 'stride':1,'pad':0, 'pool':2}]
    concatArg = {'func':Concat}
    

    # -----------Module 1: Train PixelHop -----------
    # TODO: fill in this part
    
    p2 = Pixelhop(depth=3, TH1=0.005, TH2=0.001, SaabArgs=SaabArgs, shrinkArgs=shrinkArgs, concatArg=concatArg)
    p2.fit(x_train_reduced)

    
    # --------- Module 2: get only Hop 3 feature for both training set and testing set -----------
    # you can get feature "batch wise" and concatenate them if your memory is restricted
    # TODO: fill in this part
    split_train_arr=[]
    
    for i in range(6):
        temp=get_feat(x_train[i*BS:(i+1)*BS,:,:,:],3)
        split_train_arr.append(temp)
    train_hop3_feats=np.concatenate((split_train_arr[0],split_train_arr[1],split_train_arr[2],split_train_arr[3]
                                     ,split_train_arr[4],split_train_arr[5]),axis=0)
    
    
    
    #train_hop3_feats=get_feat(x_train,3)
    test_hop3_feats=get_feat(x_test,3)
    
    ### Get Model Size
    h1=get_feat(x_train_reduced,1).shape
    h2=get_feat(x_train_reduced,2).shape
    h3=get_feat(x_train_reduced,3).shape
    
    k1=h1[3]
    k2=h2[3]
    k3=h3[3]
    
    print('k1: ', k1)
    print('k2: ', k2)
    print('k3: ', k3)
    # --------- Module 2: standardization
    STD = np.std(train_hop3_feats, axis=0, keepdims=1)
    train_hop3_feats = train_hop3_feats/STD ## size: (60000,1,1,70)
    #print(np.shape(train_hop3_feats))
    #STDtest = np.std(test_hop3_feats, axis=0, keepdims=1)
    test_hop3_feats = test_hop3_feats/STD
    
    #---------- Module 3: Train XGBoost classifier on hop3 feature ---------
    start_time = time.time()
    tr_acc = []
    te_acc = []
    
    clf = xgb.XGBClassifier(n_jobs=-1,
                        objective='multi:softprob',
                        # tree_method='gpu_hist', gpu_id=None,
                        max_depth=6,n_estimators=100,
                        min_child_weight=5,gamma=5,
                        subsample=0.8,learning_rate=0.1,
                        nthread=8,colsample_bytree=1.0)
    clf.fit(np.squeeze(train_hop3_feats), np.squeeze(y_train))


    end_time = time.time()
    print('training time is ', end_time - start_time)
    # TODO: fill in the rest and report accuracy
    score = clf.score(np.squeeze(train_hop3_feats), np.squeeze(y_train))
    print('Training Set 60000 Predicted score =', score)
    
    score = clf.score(np.squeeze(test_hop3_feats), np.squeeze(y_test))
    print('Test Set Predicted score =', score)
pred = clf.predict(np.squeeze(test_hop3_feats))

sns.set()
f,ax=plt.subplots(figsize=(10, 10))
C2= 10*confusion_matrix(y_test, pred, labels=[0, 1, 2,3,4,5,6,7,8,9], normalize='all')
print(C2) # print the confusion matrix and see 
sns.heatmap(C2,annot=True,ax=ax) #draw heat map

ax.set_title('confusion matrix') # title
ax.set_xlabel('true') #x axis
ax.set_ylabel('predict') #y axis
error_image_index_list=[]
for i in range(10000):
    if y_test[i] != pred[i]:
        error_image_index_list.append(i)


print('Real Class=', y_test[error_image_index_list[1]])
print('Predicted Class=', pred[error_image_index_list[1]])

plt.imshow(x_test[error_image_index_list[1]],cmap='gray')
plt.show()
print('Real Class=', y_test[error_image_index_list[2]])
print('Predicted Class=', pred[error_image_index_list[2]])

plt.imshow(x_test[error_image_index_list[2]],cmap='gray')
plt.show()
print('Real Class=', y_test[error_image_index_list[3]])
print('Predicted Class=', pred[error_image_index_list[3]])

plt.imshow(x_test[error_image_index_list[3]],cmap='gray')
plt.show()
print('Real Class=', y_test[error_image_index_list[4]])
print('Predicted Class=', pred[error_image_index_list[4]])

plt.imshow(x_test[error_image_index_list[4]],cmap='gray')
plt.show()
# v 2021.04.12
# A generalized version of channel wise Saab
# modified from https://github.com/ChengyaoWang/PixelHop-_c-wSaab/blob/master/cwSaab.py
# Note: Depth goal may not achieved if no nodes's energy is larger than energy threshold or too few SaabArgs/shrinkArgs, (warning generates)

import numpy as np 
import gc, time
from saab import Saab

# Python Virtual Machine's Garbage Collector
def gc_invoker(func):
    def wrapper(*args, **kw):
        value = func(*args, **kw)
        gc.collect()
        time.sleep(0.5)
        return value
    return wrapper

class cwSaab():
    def __init__(self, depth=1, TH1=0.01, TH2=0.005, SaabArgs=None, shrinkArgs=None, load=False):
        self.trained = False
        self.split = False
        
        if load == False:
            assert (depth > 0), "'depth' must > 0!"
            assert (SaabArgs != None), "Need parameter 'SaabArgs'!"
            assert (shrinkArgs != None), "Need parameter 'shrinkArgs'!"
            self.depth = (int)(depth)
            self.shrinkArgs = shrinkArgs
            self.SaabArgs = SaabArgs
            self.par = {}
            self.bias = {}
            self.TH1 = TH1
            self.TH2 = TH2
            self.Energy = {}
        
            if depth > np.min([len(SaabArgs), len(shrinkArgs)]):
                self.depth = np.min([len(SaabArgs), len(shrinkArgs)])
                print("       <WARNING> Too few 'SaabArgs/shrinkArgs' to get depth %s, actual depth: %s"%(str(depth),str(self.depth)))

    @gc_invoker
    def SaabTransform(self, X, saab, layer, train=False):
        '''
        Get saab features. 
        If train==True, remove leaf nodes using TH1, only leave the intermediate node's response
        '''
        shrinkArg, SaabArg = self.shrinkArgs[layer], self.SaabArgs[layer]
        assert ('func' in shrinkArg.keys()), "shrinkArg must contain key 'func'!"
        X = shrinkArg['func'](X, shrinkArg)
        S = list(X.shape)
        X = X.reshape(-1, S[-1])
        
        if SaabArg['num_AC_kernels'] != -1:
            S[-1] = SaabArg['num_AC_kernels']
            
        transformed = saab.transform(X)
        transformed = transformed.reshape(S[0],S[1],S[2],-1)
        
        if train==True and self.SaabArgs[layer]['cw'] == True: # remove leaf nodes
            transformed = transformed[:, :, :, saab.Energy>self.TH1]
            
        return transformed
    
    @gc_invoker
    def SaabFit(self, X, layer, bias=0):
        '''Learn a saab model'''
        shrinkArg, SaabArg = self.shrinkArgs[layer], self.SaabArgs[layer]
        assert ('func' in shrinkArg.keys()), "shrinkArg must contain key 'func'!"
        X = shrinkArg['func'](X, shrinkArg)
        S = list(X.shape)
        X = X.reshape(-1, S[-1])
        saab = Saab(num_kernels=SaabArg['num_AC_kernels'], needBias=SaabArg['needBias'], bias=bias)
        saab.fit(X)
        return saab

    @gc_invoker
    def discard_nodes(self, saab):
        '''Remove discarded nodes (<TH2) from the model'''
        energy_k = saab.Energy
        discard_idx = np.argwhere(energy_k<self.TH2)
        saab.Kernels = np.delete(saab.Kernels, discard_idx, axis=0) 
        saab.Energy = np.delete(saab.Energy, discard_idx)
        saab.num_kernels -= discard_idx.size
        return saab

    @gc_invoker
    def cwSaab_1_layer(self, X, train, bias=None):
        '''cwsaab/saab transform starting for Hop-1'''
        if train == True:
            saab_cur = []
            bias_cur = []
        else:
            saab_cur = self.par['Layer'+str(0)]
            bias_cur = self.bias['Layer'+str(0)]
        transformed, eng = [], []
        
        if self.SaabArgs[0]['cw'] == True: # channel-wise saab
            S = list(X.shape)
            S[-1] = 1
            X = np.moveaxis(X, -1, 0)
            for i in range(X.shape[0]):
                X_tmp = X[i].reshape(S)
                if train == True:
                    # fit
                    saab = self.SaabFit(X_tmp, layer=0)
                    # remove discarded nodes
                    saab = self.discard_nodes(saab)
                    # store
                    saab_cur.append(saab)
                    bias_cur.append(saab.Bias_current)
                    eng.append(saab.Energy)
                    # transformed feature
                    transformed.append(self.SaabTransform(X_tmp, saab=saab, layer=0, train=True))
                else:
                    if len(saab_cur) == i:
                        break
                    transformed.append(self.SaabTransform(X_tmp, saab=saab_cur[i], layer=0))
            transformed = np.concatenate(transformed, axis=-1)
        else: # saab, not channel-wise
            if train == True:
                saab = self.SaabFit(X, layer=0)
                saab = self.discard_nodes(saab)
                saab_cur.append(saab)
                bias_cur.append(saab.Bias_current)
                eng.append(saab.Energy)
                transformed = self.SaabTransform(X, saab=saab, layer=0, train=True)
            else:
                transformed = self.SaabTransform(X, saab=saab_cur[0], layer=0)
                
        if train == True:
            self.par['Layer0'] = saab_cur
            self.bias['Layer'+str(0)] = bias_cur
            self.Energy['Layer0'] = eng
                
        return transformed

    @gc_invoker
    def cwSaab_n_layer(self, X, train, layer):
        '''cwsaab/saab transform starting from Hop-2'''
        output, eng_cur, ct, pidx = [], [], -1, 0
        S = list(X.shape)
        saab_prev = self.par['Layer'+str(layer-1)]
        bias_prev = self.bias['Layer'+str(layer-1)]

        if train == True:
            saab_cur = []
            bias_cur = []
        else:
            saab_cur = self.par['Layer'+str(layer)]
        
        if self.SaabArgs[layer]['cw'] == True: # channel-wise saab
            S[-1] = 1
            X = np.moveaxis(X, -1, 0)
            for i in range(len(saab_prev)):
                for j in range(saab_prev[i].Energy.shape[0]):
                    if train==False:
                        ct += 1 # helping index
                    if saab_prev[i].Energy[j] < self.TH1: # this is a leaf node
                        continue
                    else: # this is an intermediate node
                        if train==True:
                            ct += 1
                        
                    self.split = True
                    X_tmp = X[ct].reshape(S)
                    
                    if train == True:
                        # fit
                        saab = self.SaabFit(X_tmp, layer=layer, bias=bias_prev[i])
                        # children node's energy should be multiplied by the parent's energy
                        saab.Energy *= saab_prev[i].Energy[j]
                        # remove the discarded nodes
                        saab = self.discard_nodes(saab)
                        # store
                        saab_cur.append(saab)
                        bias_cur.append(saab.Bias_current)
                        eng_cur.append(saab.Energy) 
                        # get output features
                        out_tmp = self.SaabTransform(X_tmp, saab=saab, layer=layer, train=True)
                    else:
                        out_tmp = self.SaabTransform(X_tmp, saab=saab_cur[pidx], layer=layer)
                        pidx += 1 # helping index
                        
                    output.append(out_tmp)
                    
                    # Clean the Cache
                    X_tmp = None
                    gc.collect()
                    out_tmp = None
                    gc.collect()
                    
            output = np.concatenate(output, axis=-1)
                    
        else: # saab, not channel-wise
            if train == True:
                # fit
                saab = self.SaabFit(X, layer=layer, bias=bias_prev[0])
                # remove the discarded nodes
                saab = self.discard_nodes(saab)
                # store
                saab_cur.append(saab)
                bias_cur.append(saab.Bias_current)
                eng_cur.append(saab.Energy)
                # get output features
                output = self.SaabTransform(X, saab=saab, layer=layer, train=True)
            else:
                output = self.SaabTransform(X, saab=saab_cur[0], layer=layer)

        if train == True:   
            if self.split == True or self.SaabArgs[0]['cw'] == False:
                self.par['Layer'+str(layer)] = saab_cur
                self.bias['Layer'+str(layer)] = bias_cur
                self.Energy['Layer'+str(layer)] = eng_cur
        
        return output
    
    def fit(self, X):
        '''train and learn cwsaab/saab kernels'''
        X = self.cwSaab_1_layer(X, train=True)
        print('=' * 45 + '>c/w Saab Train Hop 1')
        for i in range(1, self.depth):
            X = self.cwSaab_n_layer(X, train = True, layer = i)
            if (self.split == False and self.SaabArgs[i]['cw'] == True):
                self.depth = i
                print("       <WARNING> Cannot futher split, actual depth: %s"%str(i))
                break
            print('=' * 45 + f'>c/w Saab Train Hop {i+1}')
            self.split = False
        self.trained = True

    def transform(self, X):
        '''
        Get feature for all the Hops
        Parameters
        ----------
        X: Input image (N, H, W, C), C=1 for grayscale, C=3 for color image
        Returns
        -------
        output: a list of transformed feature maps
        '''
        assert (self.trained == True), "Must call fit first!"
        output = []
        X = self.cwSaab_1_layer(X, train = False)
        output.append(X)
        
        for i in range(1, self.depth):
            X = self.cwSaab_n_layer(X, train=False, layer=i)
            output.append(X)
            
        return output
    
    def transform_singleHop(self, X, layer=0):
        '''
        Get feature for a single Hop
        Parameters
        ----------
        X: previous Hops output (N, H1, W1, C1)
        layer: Hop index (start with 0)
        
        Returns
        -------
        output: transformed feature maps (N, H2, W2, C2)
        '''
        assert (self.trained == True), "Must call fit first!"
        if layer==0:
            output = self.cwSaab_1_layer(X, train = False)
        else:
            output = self.cwSaab_n_layer(X, train=False, layer=layer)
            
        return output
    
if __name__ == "__main__":
    import warnings
    warnings.filterwarnings("ignore")
    from sklearn import datasets
    from skimage.util import view_as_windows
    
    # example callback function for collecting patches and its inverse
    def Shrink(X, shrinkArg):
        win = shrinkArg['win']
        stride = shrinkArg['stride']
        ch = X.shape[-1]
        X = view_as_windows(X, (1,win,win,ch), (1,stride,stride,ch))
        return X.reshape(X.shape[0], X.shape[1], X.shape[2], -1)

    # read data
    print(" > This is a test example: ")
    digits = datasets.load_digits()
    X = digits.images.reshape((len(digits.images), 8, 8, 1))
    print(" input feature shape: %s"%str(X.shape))

    # set args
    SaabArgs = [{'num_AC_kernels':-1, 'needBias':False, 'cw': False},
                {'num_AC_kernels':-1, 'needBias':True, 'cw':True}] 
    shrinkArgs = [{'func':Shrink, 'win':2, 'stride': 2}, 
                {'func': Shrink, 'win':2, 'stride': 2}]

    print(" -----> depth=2")
    cwsaab = cwSaab(depth=2, TH1=0.001,TH2=0.0005, SaabArgs=SaabArgs, shrinkArgs=shrinkArgs)
    cwsaab.fit(X)
    output1 = cwsaab.transform(X)
    output2 = cwsaab.transform_singleHop(X)
# v 2021.04.12
# PixelHop and PixelHop++ (Module 1)
# modified from https://github.com/ChengyaoWang/PixelHop-_c-wSaab/blob/master/pixelhop2.py

from cwSaab import cwSaab
import pickle

class Pixelhop(cwSaab):
    def __init__(self, depth=1, TH1=0.005, TH2=0.001, SaabArgs=None, shrinkArgs=None, concatArg=None, load=False):
        super().__init__(depth=depth, SaabArgs=SaabArgs, shrinkArgs=shrinkArgs, load=load) #TH1=TH1, TH2=TH2, 
        self.TH1 = TH1
        self.TH2 = TH2
        self.idx = []        
        self.concatArg = concatArg

    def fit(self, X):
        super().fit(X)
        return self

    def transform(self, X):
        X = super().transform(X)
        return self.concatArg['func'](X, self.concatArg)

    def transform_singleHop(self, X, layer=0):
        X = super().transform_singleHop(X, layer=layer)
        return X

    '''Methods for Saving & Loading'''
    def save(self, filename: str):
        assert (self.trained == True), "Need to Train First"
        pixelhop_model = {}
        pixelhop_model['par'] = self.par
        pixelhop_model['bias'] = self.bias
        pixelhop_model['depth'] = self.depth
        pixelhop_model['energy'] = self.Energy
        pixelhop_model['SaabArgs'] = self.SaabArgs
        pixelhop_model['shrinkArgs'] = self.shrinkArgs
        pixelhop_model['concatArgs'] = self.concatArg
        pixelhop_model['TH1'] = self.TH1
        pixelhop_model['TH2'] = self.TH2

        with open(filename + '.pkl','wb') as f:
            pickle.dump(pixelhop_model, f)
        return

    def load(self, filename: str):
        pixelhop_model = pickle.load(open(filename + '.pkl','rb'))
        self.par = pixelhop_model['par']
        self.bias = pixelhop_model['bias']
        self.depth = pixelhop_model['depth']
        self.Energy = pixelhop_model['energy']
        self.SaabArgs = pixelhop_model['SaabArgs']
        self.shrinkArgs = pixelhop_model['shrinkArgs']
        self.concatArg = pixelhop_model['concatArgs']
        self.trained = True
        self.TH1 = pixelhop_model['TH1']
        self.TH2 = pixelhop_model['TH2']
        
        return self

if __name__ == "__main__":
    import warnings
    warnings.filterwarnings("ignore")
    from sklearn import datasets
    from skimage.util import view_as_windows

    # example callback function for collecting patches and its inverse
    def Shrink(X, shrinkArg):
        win = shrinkArg['win']
        X = view_as_windows(X, (1, win, win, 1), (1, win, win, 1))
        return X.reshape(X.shape[0], X.shape[1], X.shape[2], -1)

    # example callback function for how to concate features from different hops
    def Concat(X, concatArg):
        return X

    # read data
    print(" > This is a test example: ")
    digits = datasets.load_digits()
    X = digits.images.reshape((len(digits.images), 8, 8, 1))
    print(" input feature shape: %s"%str(X.shape))

    # set args
    SaabArgs = [{'num_AC_kernels':-1, 'needBias':False, 'cw': False},
                {'num_AC_kernels':-1, 'needBias':True, 'cw':True}] 
    shrinkArgs = [{'func':Shrink, 'win':2, 'stride': 2}, 
                {'func': Shrink, 'win':2, 'stride': 2}]
    concatArg = {'func':Concat}

    print(" --> test inv")
    print(" -----> depth=2")
    p2 = Pixelhop(depth=2, TH1=0.005, TH2=0.001, SaabArgs=SaabArgs, shrinkArgs=shrinkArgs, concatArg=concatArg)
    p2.fit(X)
    output1 = p2.transform(X)
    output2 = p2.transform_singleHop(X)

    '''Test for Save / Load'''
    p2.save('./dummy')
    p2_new = Pixelhop(load=True).load('./dummy')
    output1_new = p2_new.transform(X)
    output2_new = p2_new.transform_singleHop(X)

    print("------- DONE -------\n")

# v 2021.04.12
# Saab transformation
# modified from https://github.com/ChengyaoWang/PixelHop-_c-wSaab/blob/master/saab.py

import numpy as np
import numba
# from sklearn.decomposition import PCA, IncrementalPCA


@numba.jit(nopython = True, parallel = True)
def pca_cal(X: np.ndarray):
    cov = X.transpose() @ X
    eva, eve = np.linalg.eigh(cov)
    inds = eva.argsort()[::-1]
    eva = eva[inds]
    kernels = eve.transpose()[inds]
    return kernels, eva / (X.shape[0] - 1)

@numba.jit(forceobj = True, parallel = True)
def remove_mean(X: np.ndarray, feature_mean: np.ndarray):
    return X - feature_mean

@numba.jit(nopython = True, parallel = True)
def feat_transform(X: np.ndarray, kernel: np.ndarray):
    return X @ kernel.transpose()


class Saab():
    def __init__(self, num_kernels=-1, needBias=True, bias=0):
        self.num_kernels = num_kernels 
        self.needBias = needBias
        self.Bias_previous = bias # bias calculated from previous
        self.Bias_current = [] # bias for the current Hop
        self.Kernels = []
        self.Mean0 = [] # feature mean of AC
        self.Energy = [] # kernel energy list
        self.trained = False

    def fit(self, X): 
        assert (len(X.shape) == 2), "Input must be a 2D array!"
        X = X.astype('float32')
        
        # add bias from the previous Hop
        if self.needBias == True:
            X += self.Bias_previous
            
        # remove DC, get AC components
        dc = np.mean(X, axis = 1, keepdims = True)
        X = remove_mean(X, dc)
        
        # calcualte bias at the current Hop
        self.Bias_current = np.max(np.linalg.norm(X, axis=1))
        
        # remove feature mean --> self.Mean0
        self.Mean0 = np.mean(X, axis = 0, keepdims = True)
        X = remove_mean(X, self.Mean0)

        if self.num_kernels == -1:
            self.num_kernels = X.shape[-1]
        
        # Rewritten PCA Using Numpy
        kernels, eva = pca_cal(X)
        
        # Concatenate with DC kernel
        dc_kernel = 1 / np.sqrt(X.shape[-1]) * np.ones((1, X.shape[-1]))# / np.sqrt(largest_ev)
        kernels = np.concatenate((dc_kernel, kernels[:-1]), axis = 0)
        
        # Concatenate with DC energy
        largest_ev = np.var(dc * np.sqrt(X.shape[-1]))  
        energy = np.concatenate((np.array([largest_ev]), eva[:-1]), axis = 0)
        energy = energy / np.sum(energy)
        
        # store
        self.Kernels, self.Energy = kernels.astype('float32'), energy
        self.trained = True


    def transform(self, X):
        assert (self.trained == True), "Must call fit first!"
        X = X.astype('float32')
        
        # add bias from the previous Hop
        if self.needBias == True:
            X += self.Bias_previous
            
        # remove feature mean of AC
        X = remove_mean(X, self.Mean0)
        
        # convolve with DC and AC filters
        X = feat_transform(X, self.Kernels)
        
        return X
    
    
if __name__ == "__main__":
    from sklearn import datasets
    import warnings
    warnings.filterwarnings("ignore")
    
    print(" > This is a test example: ")
    digits = datasets.load_digits()
    data = digits.images.reshape((len(digits.images), 8, 8, 1))
    print(" input feature shape: %s"%str(data.shape))
        
    X = data.copy()
    X = X.reshape(X.shape[0], -1)[0:100]
    
    saab = Saab(num_kernels=-1, needBias=True, bias=0)
    saab.fit(X)
    
    Xt = saab.transform(X)
